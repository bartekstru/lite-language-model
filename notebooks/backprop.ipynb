{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "encode = lambda c: chars.index(c)\n",
    "decode = lambda i: chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for c in w + '.':\n",
    "            ix = encode(c)\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    exact = torch.all(dt == t.grad).item()\n",
    "    approx = torch.allclose(dt, t.grad)\n",
    "    max_diff = (torch.abs(dt - t.grad).abs().max().item())\n",
    "    print(f'{s:15s} exact: {str(exact):5s} approx: {str(approx):5s} max_diff: {max_diff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass. Because when everything is zero the\n",
    "# expression of gradient is simplified.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (n,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "print(Xb.shape, Yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1205, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# zero center the batch\n",
    "bndiff = hprebn - bnmeani\n",
    "# variance is the sum of the squared differences of examples in the batch from the mean, divided by n-1 (Bessel's correction, divide by n-1 instead of n to improve the estimate of the population variance)\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(dim=0, keepdim=True)\n",
    "# standard deviation is the square root of the variance\n",
    "# get the inverse of the standard deviation summed with a small constant to avoid division by zero\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# scale the centered batch by the inverse of the standard deviation\n",
    "bnraw = bndiff * bnvar_inv\n",
    "# apply learned gain and bias to the scaled batch\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# apply the tanh activation function\n",
    "h = torch.tanh(hpreact)\n",
    "# calculate the logits for the output layer\n",
    "logits = h @ W2 + b2\n",
    "# for numerical stability, subtract the maximum logit value in each row\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean() # we first index by the barch index and then for that particular batch row we take the probability of the character oobserved in the dataset = F.one_hot(Yb, vocab_size) * -1.0/n # n * vocab_size\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in reversed([emb, embcat,hprebn, bnmeani, bndiff, bndiff2, bnvar, bnvar_inv, bnraw, hpreact, h, logits, logit_maxes, norm_logits, counts, counts_sum, counts_sum_inv, probs, logprobs]):\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        exact: True  approx: True  max_diff: 0.0\n",
      "probs           exact: True  approx: True  max_diff: 0.0\n",
      "counts_sum_inv  exact: True  approx: True  max_diff: 0.0\n",
      "counts_sum      exact: True  approx: True  max_diff: 0.0\n",
      "counts          exact: True  approx: True  max_diff: 0.0\n",
      "norm_logits     exact: True  approx: True  max_diff: 0.0\n",
      "logit_maxes     exact: True  approx: True  max_diff: 0.0\n",
      "logits          exact: True  approx: True  max_diff: 0.0\n",
      "h               exact: True  approx: True  max_diff: 0.0\n",
      "W2              exact: True  approx: True  max_diff: 0.0\n",
      "b2              exact: True  approx: True  max_diff: 0.0\n",
      "hpreact         exact: True  approx: True  max_diff: 0.0\n",
      "bngain          exact: True  approx: True  max_diff: 0.0\n",
      "bnraw           exact: True  approx: True  max_diff: 0.0\n",
      "dbnbias         exact: True  approx: True  max_diff: 0.0\n",
      "dbnvar_inv      exact: True  approx: True  max_diff: 0.0\n",
      "dbnvar          exact: True  approx: True  max_diff: 0.0\n",
      "dbndiff2        exact: True  approx: True  max_diff: 0.0\n",
      "dbndiff         exact: True  approx: True  max_diff: 0.0\n",
      "dbnmeani        exact: True  approx: True  max_diff: 0.0\n",
      "dhprebn         exact: True  approx: True  max_diff: 0.0\n",
      "dembcat         exact: True  approx: True  max_diff: 0.0\n",
      "dW1             exact: True  approx: True  max_diff: 0.0\n",
      "db1             exact: True  approx: True  max_diff: 0.0\n",
      "demb            exact: True  approx: True  max_diff: 0.0\n",
      "C               exact: True  approx: True  max_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = F.one_hot(Yb, vocab_size) * -1.0/n                                  # n * vocab_size\n",
    "dprobs = dlogprobs * (1.0/probs)                                                # n * vocab_size\n",
    "dcounts = dprobs * counts_sum_inv                                               # n * vocab_size\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True)                        # n * 1\n",
    "dcounts_sum = dcounts_sum_inv * -1 * (counts_sum ** -2)                         # n * 1\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum                                # n * vocab_size\n",
    "dnorm_logits = dcounts * counts                                                 # n * vocab_size\n",
    "dlogits = dnorm_logits.clone()                                                  # n * vocab_size    \n",
    "dlogit_maxes = - dnorm_logits.sum(1, keepdim=True)                              # n * 1\n",
    "dlogits  += dlogit_maxes * F.one_hot(logits.max(dim=1).indices, vocab_size)     # n * vocab_size\n",
    "dh = dlogits @ W2.T                                                             # n * n_hidden\n",
    "dW2 = h.T @ dlogits                                                             # n_hidden * vocab_size\n",
    "db2 = dlogits.sum(0)                                                            # vocab_size\n",
    "dhpreact = (1.0 - h**2) * dh                                                    # n * n_hidden\n",
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)                               # 1 * n_hidden\n",
    "dbnraw = dhpreact * bngain                                                      # n * n_hidden\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)                                         # 1 * n_hidden           \n",
    "dbndiff = dbnraw * bnvar_inv                                                    # n * n_hidden\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)                             # 1 * n_hidden\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv                               # 1 * n_hidden\n",
    "dbndiff2 = torch.ones_like(bndiff2) * (1.0/(n-1)) * dbnvar                      # n * n_hidden\n",
    "dbndiff += dbndiff2 * 2 * bndiff                                                # n * n_hidden\n",
    "dhprebn = dbndiff.clone()                                                       # n * n_hidden                             \n",
    "dbnmeani = (-dbndiff).sum(0)                                                    # 1 * n_hidden\n",
    "dhprebn += (1/n) * torch.ones_like(hprebn) * dbnmeani                           # n * n_hidden\n",
    "dembcat = dhprebn @ W1.T                                                        # n * (n_embd * block_size)\n",
    "dW1 = embcat.T @ dhprebn                                                        # (n_embd * block_size) * n_hidden\n",
    "db1 = dhprebn.sum(0)                                                            # n_hidden\n",
    "demb = dembcat.view(emb.shape)                                                  # n * (block_size * n_embd)\n",
    "\n",
    "X_flatten = Xb.view(-1) \n",
    "demb_flatten = demb.view(-1, n_embd)\n",
    "dC = torch.zeros_like(C)\n",
    "dC.index_add_(0, X_flatten, demb_flatten)                                       # vocab_size * n_embd\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbnvar', dbnvar, bnvar)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Derivative\n",
    "\n",
    "Here are two images illustrating the derivative of the batch normalization layer:\n",
    "\n",
    "![Batch Normalization Derivative 1](./images/bn_derivative1.jpeg)\n",
    "\n",
    "![Batch Normalization Derivative 2](./images/bn_derivative2.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y               exact: False approx: True  max_diff: 1.862645149230957e-09\n"
     ]
    }
   ],
   "source": [
    "# manual baclprop thorugh the batchnorm layer: dL/dhprebn\n",
    "a = (bngain * (bnvar + 1e-05) **-0.5) / n\n",
    "b = n * hpreact.grad\n",
    "c = hpreact.grad.sum(dim=0, keepdim=True)\n",
    "d = (bnraw * hpreact.grad).sum(dim=0, keepdim=True)\n",
    "e = (n/(n-1))*bnraw*(bnraw * hpreact.grad).sum(dim=0, keepdim=True)\n",
    "y = a * (b - c - e)\n",
    "y = (bngain * (bnvar + 1e-05) **-0.5) / n * (n * hpreact.grad - hpreact.grad.sum(dim=0, keepdim=True) - (n/(n-1))*bnraw*(bnraw * hpreact.grad).sum(dim=0, keepdim=True))\n",
    "\n",
    "cmp('y', y, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits_fast    exact: False approx: True  max_diff: 6.05359673500061e-09\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "dlogits_fast = (F.softmax(logits, 1) - F.one_hot(Yb, vocab_size)) / n\n",
    "cmp('dlogits_fast', dlogits_fast, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15e74d2f9d0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0670, 0.0856, 0.0172, 0.0502, 0.0199, 0.0873, 0.0250, 0.0385, 0.0197,\n",
       "        0.0304, 0.0359, 0.0358, 0.0356, 0.0280, 0.0321, 0.0135, 0.0096, 0.0185,\n",
       "        0.0158, 0.0548, 0.0529, 0.0218, 0.0248, 0.0718, 0.0613, 0.0252, 0.0217],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0670,  0.0856,  0.0172,  0.0502,  0.0199,  0.0873,  0.0250,  0.0385,\n",
       "         -0.9803,  0.0304,  0.0359,  0.0358,  0.0356,  0.0280,  0.0321,  0.0135,\n",
       "          0.0096,  0.0185,  0.0158,  0.0548,  0.0529,  0.0218,  0.0248,  0.0718,\n",
       "          0.0613,  0.0252,  0.0217], grad_fn=<MulBackward0>),\n",
       " tensor(-7.4506e-09, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dlogits[0] * n # times n to undo the mean\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to increase the probability of the the right character. Increasing the probability of the righ character means decreaing the counts of the incorrect characters and increasing the counts of the right character. So we will add to the logit of the correct character (-(-0.9803) -> 0.9803) and we will subtract from the logits of the incorrect characters. And the magnitude of the force of the pull upwards (for the correct character) and the force of the pull downwards will be the same (the sum above is zero). So we are pulling and pushing and the amount of the force that we apply is proportional to the probabilities that came out in the forward pass. So if one character was predicted as likely but was actually not, it will get updated more than one that was incorrect but also had low probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3413\n",
      "  10000/ 200000: 2.4379\n",
      "  20000/ 200000: 2.1176\n",
      "  30000/ 200000: 2.3711\n",
      "  40000/ 200000: 2.2701\n",
      "  50000/ 200000: 2.2300\n",
      "  60000/ 200000: 2.2726\n",
      "  70000/ 200000: 2.5170\n",
      "  80000/ 200000: 2.1190\n",
      "  90000/ 200000: 2.3738\n",
      " 100000/ 200000: 2.0856\n",
      " 110000/ 200000: 1.8341\n",
      " 120000/ 200000: 1.9820\n",
      " 130000/ 200000: 2.5408\n",
      " 140000/ 200000: 1.8672\n",
      " 150000/ 200000: 2.1628\n",
      " 160000/ 200000: 2.0586\n",
      " 170000/ 200000: 2.2303\n",
      " 180000/ 200000: 1.9925\n",
      " 190000/ 200000: 2.0217\n",
      " 199999/ 200000: 2.0544\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass. Because when everything is zero the\n",
    "# expression of gradient is simplified.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "max_steps = 200000\n",
    "eval_interval = 10000\n",
    "batch_size = 32\n",
    "n = batch_size\n",
    "lossi = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (n,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # -------------------------- FORWARD PASS -------------------------- #\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hprebn = embcat @ W1 + b1\n",
    "\n",
    "    # -------------------------- BATCHNORM LAYER ----------------------- #\n",
    "    bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "    bndiff = hprebn - bnmeani\n",
    "    bndiff2 = bndiff**2\n",
    "    bnvar = 1/(n-1) * bndiff2.sum(dim=0, keepdim=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = bndiff * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------- ACTIVATION FUNCTION ------------------- #\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    # -------------------------- LOSS FUNCTION ------------------------- #\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # -------------------------- BACKWARD PASS ------------------------- #\n",
    "    dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "    dlogits = (F.softmax(logits, 1) - F.one_hot(Yb, vocab_size)) / n\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "    dhprebn = bngain * bnvar_inv / n * (n * dhpreact - dhpreact.sum(dim=0, keepdim=True) - (n/(n-1))*bnraw*(bnraw * dhpreact).sum(dim=0, keepdim=True))\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    demb = dembcat.view(emb.shape)\n",
    "\n",
    "    X_flatten = Xb.view(-1)\n",
    "    demb_flatten = demb.view(-1, n_embd)\n",
    "    dC = torch.zeros_like(C)\n",
    "    dC.index_add_(0, X_flatten, demb_flatten)\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "    # -------------------------- UPDATE PARAMETERS ----------------------- #\n",
    "    lr = 0.1 if i < max_steps * 0.5 else 0.01\n",
    "\n",
    "    for p, d in zip(parameters, grads):\n",
    "      p.data -= lr * d\n",
    "\n",
    "    if i % eval_interval == 0 or i == max_steps - 1:\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "      lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # pass the entire training set thorugh the network\n",
    "    # measure the mean and the standard deviation of hpreact for the entire training set\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(dim=0, keepdim=True)\n",
    "    bnvar = hpreact.var(dim=0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1457653045654297\n",
      "val 2.163302183151245\n",
      "test 2.1645994186401367\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xval, Yval),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = x @ W1 + b1\n",
    "  hpreact_centered = hpreact - bnmean\n",
    "  hpreact_normalized = bngain * (hpreact_centered * (bnvar + 1e-5)**-0.5) + bnbias\n",
    "  h = torch.tanh(hpreact_normalized)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fini.\n",
      "sam.\n",
      "kayanni.\n",
      "jarjanurryah.\n",
      "emmurhan.\n",
      "asme.\n",
      "prehrunte.\n",
      "ron.\n",
      "lenselvelli.\n",
      "prashs.\n",
      "savarebrirynnnlee.\n",
      "phir.\n",
      "dresh.\n",
      "dinashutrevyn.\n",
      "mehnennie.\n",
      "naigertonnveen.\n",
      "jacharlinna.\n",
      "laniniqunne.\n",
      "jayve.\n",
      "nhyndrnnyth.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    context = [0] * block_size\n",
    "    out = []\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "        hpreact = x @ W1 + b1\n",
    "        hpreact_centered = hpreact - bnmean\n",
    "        hpreact_normalized = bngain * (hpreact_centered * (bnvar + 1e-5)**-0.5) + bnbias\n",
    "        h = torch.tanh(hpreact_normalized)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(decode(ix))\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
