{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "encode = lambda c: chars.index(c)\n",
    "decode = lambda i: chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for c in w + '.':\n",
    "            ix = encode(c)\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 5/3 * (n_embd * block_size)**-0.5\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.01 useless because of batch norm's layer own bias\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.zeros(n_hidden)\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.1984\n",
      "  20000/ 200000: 2.3375\n",
      "  30000/ 200000: 2.4359\n",
      "  40000/ 200000: 2.0119\n",
      "  50000/ 200000: 2.2595\n",
      "  60000/ 200000: 2.4775\n",
      "  70000/ 200000: 2.1020\n",
      "  80000/ 200000: 2.2788\n",
      "  90000/ 200000: 2.1862\n",
      " 100000/ 200000: 1.9474\n",
      " 110000/ 200000: 2.3010\n",
      " 120000/ 200000: 1.9837\n",
      " 130000/ 200000: 2.4523\n",
      " 140000/ 200000: 2.3839\n",
      " 150000/ 200000: 2.1987\n",
      " 160000/ 200000: 1.9733\n",
      " 170000/ 200000: 1.8668\n",
      " 180000/ 200000: 1.9973\n",
      " 190000/ 200000: 1.8347\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 # + b1 removing b1 because bn layer will have its own # hidden layer pre-activation \n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad(): # keeo the running stats\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2254643f410>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    hpreact = x @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3355,  0.6776, -0.9133,  1.0163,  1.0865,  1.0937,  1.7437, -2.1208,\n",
       "          0.5730,  1.4455, -1.6343, -2.7372, -0.4752, -0.1412, -0.0745, -1.1722,\n",
       "          0.6851, -2.6219, -0.1065,  1.6326, -0.7706, -0.3063,  0.0479,  0.6116,\n",
       "          1.1173,  0.2427,  2.0500,  0.5831,  0.8527,  1.7680, -0.3625, -0.8355,\n",
       "         -0.0854, -0.5177, -0.3806, -1.0699, -0.0786,  0.3487, -0.5808,  0.9875,\n",
       "         -0.4427, -1.3082, -0.2871, -0.2332,  0.6850,  0.6850,  2.0857, -0.7608,\n",
       "          2.3866,  1.8734,  0.8259,  0.2803,  1.8897,  0.4709,  0.6739, -1.8940,\n",
       "         -0.0401,  0.4338,  1.3760, -0.8910, -0.4523,  1.1754,  0.5613,  0.6051,\n",
       "          1.5858,  1.2261, -1.0111,  2.1495, -0.6393,  0.0938, -0.2864, -0.4856,\n",
       "          0.9632, -1.0461, -2.9990,  0.6391,  1.4327, -0.1590,  0.0941,  0.5253,\n",
       "          0.2508,  1.2521,  2.0388,  0.6608,  0.0691, -0.0813, -1.6723,  0.2933,\n",
       "          2.2423, -0.0210, -0.6666,  1.4253, -0.8412, -1.2248, -1.0129,  0.2230,\n",
       "          0.2112, -0.3226,  0.1141, -0.6350,  0.1848,  0.1165, -1.3947,  0.2343,\n",
       "          0.2170, -0.3494, -0.3409, -0.1918,  0.9244, -0.8073,  0.7147,  0.2383,\n",
       "          0.3964,  1.2354,  2.8623,  2.0470,  0.8419,  0.8480,  0.3483, -0.3153,\n",
       "         -1.0647, -1.4465,  0.3518,  1.1174, -1.1265,  0.0688, -0.2204, -0.4964,\n",
       "         -0.8558, -1.0849,  2.8098, -1.2867,  0.6543,  1.8674,  0.9511,  1.0606,\n",
       "         -0.7020,  1.8505,  0.1528,  0.4583,  1.6235, -0.1899,  1.9335,  0.2938,\n",
       "          0.4391,  0.2065,  1.1369, -0.6884,  0.1559,  0.5905, -1.5956, -0.2324,\n",
       "          1.7724,  1.2664,  0.9916, -0.4957,  1.4023, -1.6664,  0.0178,  0.4224,\n",
       "          0.5857, -0.4225,  0.9377, -1.1793, -0.4486, -0.7184, -0.3307, -0.4207,\n",
       "         -2.1519, -0.1099,  1.1733, -2.1530,  0.1215, -0.8929, -0.9959,  0.9028,\n",
       "          1.3427, -0.7526,  1.3065, -1.5016,  1.8439,  0.9704,  0.8244, -0.7247,\n",
       "          0.3621,  0.0768, -0.9657,  0.1345,  1.8784,  0.9191, -0.5020,  1.6438,\n",
       "         -0.7394,  0.2081,  0.1425,  1.2711,  1.8083, -0.8162,  0.2586,  1.2403]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3539,  0.6872, -0.9001,  1.0159,  1.0894,  1.0862,  1.7389, -2.1357,\n",
       "          0.5608,  1.4246, -1.6445, -2.7426, -0.4861, -0.1510, -0.0687, -1.1550,\n",
       "          0.6891, -2.6399, -0.1283,  1.6240, -0.7732, -0.2865,  0.0467,  0.6119,\n",
       "          1.1172,  0.2433,  2.0542,  0.5778,  0.8515,  1.7729, -0.3741, -0.8385,\n",
       "         -0.0831, -0.5198, -0.3817, -1.0699, -0.0780,  0.3370, -0.5768,  0.9935,\n",
       "         -0.4507, -1.3313, -0.2895, -0.2299,  0.6877,  0.6936,  2.0835, -0.7759,\n",
       "          2.3804,  1.8613,  0.8117,  0.2735,  1.8802,  0.4705,  0.6656, -1.8962,\n",
       "         -0.0420,  0.4356,  1.3924, -0.8906, -0.4676,  1.1688,  0.5539,  0.6001,\n",
       "          1.5853,  1.2103, -1.0171,  2.1422, -0.6330,  0.1071, -0.2926, -0.4831,\n",
       "          0.9506, -1.0144, -2.9925,  0.6268,  1.4404, -0.1574,  0.0955,  0.5159,\n",
       "          0.2487,  1.2401,  2.0104,  0.6695,  0.0768, -0.0851, -1.6768,  0.2963,\n",
       "          2.2374, -0.0100, -0.6669,  1.4356, -0.8431, -1.2317, -1.0220,  0.2201,\n",
       "          0.1928, -0.3261,  0.1108, -0.6206,  0.1795,  0.1089, -1.4007,  0.2215,\n",
       "          0.2301, -0.3369, -0.3340, -0.1849,  0.9342, -0.8288,  0.7118,  0.2475,\n",
       "          0.3813,  1.2447,  2.8428,  2.0338,  0.8295,  0.8458,  0.3484, -0.3078,\n",
       "         -1.0773, -1.4394,  0.3424,  1.1274, -1.1252,  0.0692, -0.2308, -0.4936,\n",
       "         -0.8471, -1.0785,  2.8080, -1.2824,  0.6541,  1.8701,  0.9662,  1.0696,\n",
       "         -0.7051,  1.8326,  0.1624,  0.4782,  1.6376, -0.1962,  1.9364,  0.2986,\n",
       "          0.4300,  0.2062,  1.1390, -0.6907,  0.1571,  0.5883, -1.5947, -0.2229,\n",
       "          1.7796,  1.2689,  0.9813, -0.4950,  1.4052, -1.6645,  0.0249,  0.4152,\n",
       "          0.5742, -0.4044,  0.9335, -1.1684, -0.4574, -0.7229, -0.3192, -0.4214,\n",
       "         -2.1397, -0.1036,  1.1620, -2.1602,  0.1196, -0.8847, -0.9970,  0.9115,\n",
       "          1.3360, -0.7411,  1.2991, -1.5061,  1.8279,  0.9732,  0.8192, -0.7259,\n",
       "          0.3556,  0.0714, -0.9718,  0.1299,  1.8912,  0.9107, -0.4950,  1.6493,\n",
       "         -0.7601,  0.2008,  0.1606,  1.2586,  1.8110, -0.8021,  0.2453,  1.2369]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmean_running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.06659197807312\n",
      "val 2.1050572395324707\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xval, Yval),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = x @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When \n",
    "```\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "```\n",
    "<pre>\n",
    "      \n",
    "      0/ 200000: 27.8817 <------- this is way to high, means the initialization is wrong. you sholud have a roiugh idea of what the initial loss should be. The network is very confidentl wrog\n",
    "  10000/ 200000: 2.8547  <- at init the distribvutoin should be uniform. p=1/27, nll = -log(1/27) = 3.29. this should be roughly the starting point. the initial logits should be roughly the same for all chars\n",
    "  20000/ 200000: 2.5158\n",
    "  30000/ 200000: 2.8251\n",
    "  40000/ 200000: 2.0468\n",
    "  50000/ 200000: 2.5537\n",
    "  60000/ 200000: 2.3718\n",
    "  70000/ 200000: 2.1337\n",
    "  80000/ 200000: 2.2612\n",
    "  90000/ 200000: 2.3405\n",
    " 100000/ 200000: 2.0309\n",
    " 110000/ 200000: 2.4856\n",
    " 120000/ 200000: 1.9046\n",
    " 130000/ 200000: 2.4561\n",
    " 140000/ 200000: 2.2440\n",
    " 150000/ 200000: 2.1515\n",
    " 160000/ 200000: 2.0846\n",
    " 170000/ 200000: 1.8103\n",
    " 180000/ 200000: 2.0295\n",
    " 190000/ 200000: 1.8398\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want all the logits to be roughly zero at iniitalization. how to achieve it?\n",
    "\n",
    "set the b2 to zero, so that it is just zero at init. also, make w2 very small, to make the logits very small. so scale it by 0.01. do not scale it by zero. with scalin like that we get a differnet starting point:\n",
    "\n",
    "<pre>\n",
    "0/ 200000: 3.3221\n",
    "  10000/ 200000: 2.1900\n",
    "  20000/ 200000: 2.4196\n",
    "  30000/ 200000: 2.6067\n",
    "</pre>\n",
    "\n",
    "we took away the easy part of the optimalization and there is no hockey stick appearance. hockey stick means that the we are wasting cycles to simply sqyash the weights.\n",
    "\n",
    "the problem is now with the activations of the hidden state. this is what the output of h after one iteration looks like:\n",
    "\n",
    "<pre>\n",
    "tensor([[ 0.8100, -0.8997, -0.9993,  ..., -0.9097, -1.0000,  1.0000],\n",
    "        [-1.0000, -0.9571, -0.7145,  ...,  0.4898,  0.9090,  0.9937],\n",
    "        [ 0.9983, -0.3340,  1.0000,  ...,  0.9443,  0.9905,  1.0000],\n",
    "        ...,\n",
    "        [-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000],\n",
    "        [-1.0000, -0.4385, -0.8882,  ..., -0.3316,  0.9995,  1.0000],\n",
    "        [-1.0000,  0.9604, -0.1418,  ..., -0.1266,  1.0000,  1.0000]],\n",
    "       grad_fn=<TanhBackward0>)\n",
    "</pre>\n",
    "\n",
    "many of its elements are 1 or close to it, whhich blocks the gradients from flowing backwards\n",
    "\n",
    "\n",
    "Let's visualize the distribution of activations in the hidden state: \n",
    "\n",
    "![Histogram of hidden state activations](images/histogams_1.png)\n",
    " \n",
    "As we can see from the histogram, many of the activations are concentrated at the extreme values of -1 and 1. This is problematic because it can lead to vanishing gradients during backpropagation, as the gradient of the tanh function is very small near these values.\n",
    "\n",
    "The distribution of the pre activation values looks like this:\n",
    "\n",
    "![Histogram of hidden state activations](images/hpreact_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the variance of preactivations is so large, a lot of inputrs to the tanh are in the flat saturated region. the gradient flowing thorugh tanh can only ever decrease by (1-t^2) factor.\n",
    "the concern is that if all the outputs are in the flat regions, the gradient s flowing backwars wil be destroyed.\n",
    "\n",
    "White pixel - dead.\n",
    "\n",
    "![Histogram of hidden state activations](images/dead_1.png)\n",
    "\n",
    "\n",
    "if the entire column is white, this means that for every example in the batch the activation is in the flat region, and the neuron is esentially dead. this neuron will never learn.\n",
    "\n",
    "at least here, for each neuron, we have some examples which trigger it.\n",
    "\n",
    "however, what can happen during training is that a high gradient knoks a neuron off into this dead space and from that point on, the neuron never learns.\n",
    "\n",
    "the lesson is that the preactivation values are too large. they need to be smaller so that the activations are not in the dead space. We want them to be roughly guassian.\n",
    "\n",
    "`hpreact = embcat @ W1 + b1 # hidden layer pre-activation`\n",
    "\n",
    "we want hpreact to be closer to zero. it is okey to set the biases to something small but not zero to introduce some entropy so that there is some var and dversity, that can help a little bit.\n",
    "we want to squash the weights too\n",
    "\n",
    "now we have \n",
    "\n",
    "```\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.1\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "```\n",
    "\n",
    "![Histogram of hidden state activations](images/dead_2.png)\n",
    "\n",
    "![Histogram of hidden state activations](images/histogams_2.png)\n",
    "\n",
    "\n",
    "and the activations are not in the flat area. \n",
    "\n",
    "The pre activations are also not that spread out.\n",
    "\n",
    "![Histogram of hidden state activations](images/hpreact_2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initial, softmax confidently wrong\n",
    "train 2.125821828842163\n",
    "val 2.1704132556915283\n",
    "\n",
    "fix softmax initially confidently wrong\n",
    "train 2.069589138031006\n",
    "val 2.131074905395508\n",
    "\n",
    "fix tanh too saturated at init\n",
    "train 2.0355966091156006\n",
    "val 2.102678060531616\n",
    "\n",
    "kaiming intialization:\n",
    "train 2.0376644134521484\n",
    "val 2.106989622116089 < we got to the same spot without having to introduce any maginc numbers\n",
    "\n",
    "with batchnorm and all previous improvements:\n",
    "train 2.0668270587921143\n",
    "val 2.104844808578491\n",
    "\n",
    "with batchnorm and running means:\n",
    "train 2.06659197807312\n",
    "val 2.1050572395324707\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1984)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(10000) * 0.2).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0022) tensor(0.9950)\n",
      "tensor(0.0012) tensor(0.6430)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) * 0.2\n",
    "y=x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True);\n",
    "plt.subplot(122)\n",
    "_ = plt.hist(y.view(-1).tolist(), 50, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the input is just a guassian, mean 0 and std of 1\n",
    "\n",
    "the std of y has increased to three\n",
    "\n",
    "```\n",
    "tensor(0.0124) tensor(0.9947) <- input\n",
    "tensor(-0.0070) tensor(3.1710) <- output\n",
    "```\n",
    "\n",
    "the output guassian has now expanded. \n",
    "\n",
    "we want most of the nn to have roughlt similar activations. so the question is, how do we scale the outputs to preserve the gaussian output?\n",
    "\n",
    "lets multiply the weights by five `w = torch.randn(10, 200) * 5`:\n",
    "\n",
    "```\n",
    "tensor(-0.0124) tensor(1.0053)\n",
    "tensor(0.0279) tensor(15.9336)\n",
    "```\n",
    "\n",
    "lets multiply the weights by five `w = torch.randn(10, 200) * 0.2`:\n",
    "\n",
    "```\n",
    "tensor(0.0022) tensor(0.9950)\n",
    "tensor(0.0012) tensor(0.6430)\n",
    "```\n",
    "\n",
    "how do we choose the number by which to scale the weights? for a simple linear layer, divide by a square root of fan in. when you do that, when you have a roughlt guassian input, out of this layer you will have a roughly gaussian output\n",
    "\n",
    "however, with non linearities, you need to compensate for the discarded part of the distribution, so you also need a gain (look into torch.nn.init). basically the formula for the scaling factor is gain/sqrt(fanin)\n",
    "\n",
    "so the initalization becomes:\n",
    "\n",
    "```\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 5/3 * (n_embd * block_size)**-0.5\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "```\n",
    "\n",
    "another way of normalizing the outputs of the layers is batchnorm. since we want hpreact to be roughy gaussian (not to big to avoide the dead space and not to small to avoid the linear part of tanh, which makes it inactive), we can simply take the pre activations for the entire batch and normalize them.\n",
    "\n",
    "however, we only want the preactivations to be normal gaussian at initialization. we want the neural net to be able to adjust the distribution as it sees fit, during backprop. so we include scale and shift parameters:\n",
    "```\n",
    "bngain = torch.ones(n_hidden)\n",
    "bnbias = torch.zeros(n_hidden)\n",
    "```\n",
    "\n",
    "the benefit of batchnorm is that for huge neural nets with many layers, settting the right innit scaling factors becomes intractable. you cannot set everything by hand. but with explicit form of normalization, batch norm, this will be done automatically. it is common to include batchnorm after layers with mutilipcations, such as linear or conv, to control the scale of activations without manual mathematics.\n",
    "\n",
    "in the following line in the training loop\n",
    "`hpreact = embcat @ W1 + b1 # hidden layer pre-activation` \n",
    "adding the bias here is not wasteful, because the bias is removed during batchnorm and added back as the learned bias\n",
    "\n",
    "summary of bn:\n",
    "\n",
    "we are using bn to control the statistics of activations in n\n",
    "\n",
    "common to sprinkle across the net affter layers wsith muls, like linear or cnn\n",
    "\n",
    "bnin terlly has params for agin anf bias, trained with badkporp\n",
    "\n",
    "it als has buffers, not trained, for running stats\n",
    "\n",
    "bn momentum:\n",
    "\n",
    "with small batch sizes, the stats might cvhange significantly from iteration to iteration. since the value is chaning aournd a lot, you dont want to take too much from any particular batch. instead, you want to have a large smooting, so the momentum should be kept to low. with large batch sizes, it might be safe to keep the momentum relativlely high.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization: Benefits and Drawbacks\n",
    "Unexpected Stability at a Cost\n",
    "1. Batch normalization provides stability in neural networks, but at a significant cost.\n",
    "This cost involves a fundamental change in how neural networks process data.\n",
    "Traditional Neural Network Processing\n",
    "1. Originally, neural networks processed single examples deterministically.\n",
    "Batches were later introduced for training efficiency, but examples were still processed independently.\n",
    "The Batch Normalization Paradigm Shift\n",
    "1. Batch normalization couples examples within a batch mathematically.\n",
    "This coupling occurs in both forward and backward passes of the neural network.\n",
    "Impact on Network Activations\n",
    "Hidden state activations (H) and logits for any input example now depend on:\n",
    "The input example itself\n",
    "All other examples in the batch\n",
    "This leads to subtle changes or \"jittering\" in activations based on batch composition.\n",
    "Unexpected Benefits\n",
    "This \"jittering\" effect, initially seeming like a bug, turns out to be beneficial.\n",
    "It acts as a regularizer for the neural network.\n",
    "Regularization Mechanism\n",
    "The process effectively \"pads out\" input examples.\n",
    "Introduces entropy into the system.\n",
    "Acts as a form of data augmentation.\n",
    "Makes it harder for the neural network to overfit specific examples.\n",
    "Challenges in Removing Batch Normalization\n",
    "The regularization effect has made it difficult to eliminate batch normalization.\n",
    "No one likes the property of mathematically coupled examples in a batch.\n",
    "It can lead to strange results and bugs.\n",
    "Alternative Normalization Techniques\n",
    "Efforts to deprecate batch normalization in favor of other techniques that don't couple batch examples:\n",
    "Layer normalization\n",
    "Instance normalization\n",
    "Group normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0355966091156006\n",
      "val 2.102678060531616\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xval, Yval),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = x @ W1 + b1\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) * (fan_in)**-0.5\n",
    "        self.bias = torch.randn((fan_out), generator=g) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# batch norm\n",
    "# we are using batch norm to control the statistics of the activation in the nn. \n",
    "# bn is usually applied after layers with multiplications, such as FC linear or CNN layers.\n",
    "# bn has params for gain and bias, which are learnable parameters that scale and shift the output of the batch norm.\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(dim=0, keepdim=True)\n",
    "            xvar = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        self.out = (x - xmean) / (xvar + self.eps).sqrt()\n",
    "        self.out = self.out * self.gamma + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + xmean * self.momentum\n",
    "                self.running_var = self.running_var * (1 - self.momentum) + xvar * self.momentum\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize historgams for the forward pass\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xval, Yval),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "# put layers into eval mode\n",
    "for layer in layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(decode(i) for i in out)) # decode and print the generated word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
