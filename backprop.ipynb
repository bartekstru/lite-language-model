{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "encode = lambda c: chars.index(c)\n",
    "decode = lambda i: chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for c in w + '.':\n",
    "            ix = encode(c)\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    exact = torch.all(dt == t)\n",
    "    approx = torch.allclose(dt, t.grad)\n",
    "    max_diff = (torch.abs(dt - t.grad).abs().max().item())\n",
    "    print(f'{s:15s} exact: {str(exact):5s} approx: {str(approx):5s} max_diff: {max_diff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass. Because when everything is zero the\n",
    "# expression of gradient is simplified.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (n,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "print(Xb.shape, Yb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5024, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed the inputs\n",
    "emb = C[Xb] # (n, block_size, n_embd)\n",
    "embcat = emb.view(Xb.shape[0], -1) # (n, block_size * n_embd)\n",
    "# calxulate the linear layer outputs\n",
    "hprebn = embcat @ W1 + b1 # (n, n_hidden)\n",
    "# get the mean of the batch\n",
    "bnmeani = hprebn / hprebn.sum(dim=0, keepdim=True)\n",
    "# zero center the batch\n",
    "bndiff = hprebn - bnmeani\n",
    "# variance is the sum of the squared differences of examples in the batch from the mean, divided by n-1 (Bessel's correction, divide by n-1 instead of n to improve the estimate of the population variance)\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(dim=0, keepdim=True)\n",
    "# standard deviation is the square root of the variance\n",
    "# get the inverse of the standard deviation summed with a small constant to avoid division by zero\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# scale the centered batch by the inverse of the standard deviation\n",
    "bnraw = bndiff * bnvar_inv\n",
    "# apply learned gain and bias to the scaled batch\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# apply the tanh activation function\n",
    "h = torch.tanh(hpreact)\n",
    "# calculate the logits for the output layer\n",
    "logits = h @ W2 + b2\n",
    "# for numerical stability, subtract the maximum logit value in each row\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean() # we first index by the barch index and then for that particular batch row we take the probability of the character oobserved in the dataset\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in reversed([emb, embcat, hprebn, bnmeani, bndiff, bndiff2, bnvar, bnvar_inv, bnraw, hpreact, h, logits, logit_maxes, norm_logits, counts, counts_sum, counts_sum_inv, probs, logprobs]):\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (1024) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m dlogits \u001b[38;5;241m=\u001b[39m dnorm_logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      9\u001b[0m dlogits_maxes \u001b[38;5;241m=\u001b[39m dnorm_logits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m dlogits \u001b[38;5;241m=\u001b[39m \u001b[43mdlogits_maxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (1024) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "dlogprobs = -1/n * F.one_hot(Yb, vocab_size) # 32 x 27\n",
    "dprobs = dlogprobs * (probs**-1)\n",
    "dcounts = dprobs * counts_sum_inv\n",
    "dcounts_sum_inv = dprobs * counts\n",
    "dcounts_sum = dcounts_sum_inv * -1 * (counts_sum ** -2)\n",
    "dcounts += dcounts_sum * counts\n",
    "dnorm_logits = dcounts * norm_logits.exp()\n",
    "dlogits = dnorm_logits * 1\n",
    "dlogits_maxes = dnorm_logits * -1\n",
    "# dlogits = dlogits_maxes * logits[range(n), logits.argmax(dim=1, keepdim=True)].view((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogprobs has to be the same size as loss -> 32 x 27\n",
    "dlogprobs = -(1/n) * torch.zerows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4843, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1837, -2.7647, -3.8949, -3.4204, -3.5088, -2.3889, -3.4442, -3.3123,\n",
       "         -4.0456, -3.9576, -3.0628, -3.4465, -3.0465, -3.2209, -2.9837, -3.9360,\n",
       "         -4.2538, -4.4107, -3.7310, -2.5961, -3.1221, -4.0849, -4.0183, -2.7534,\n",
       "         -2.7924, -3.7079, -3.6333],\n",
       "        [-3.3363, -3.0017, -2.3151, -3.2497, -2.7663, -3.9893, -3.7965, -3.6317,\n",
       "         -3.8491, -3.5038, -3.2904, -3.4172, -3.1890, -2.9364, -2.6960, -2.8940,\n",
       "         -3.6866, -3.9671, -4.3571, -2.9187, -3.7511, -4.6416, -4.3379, -2.6600,\n",
       "         -3.7620, -3.6106, -3.4701],\n",
       "        [-4.3264, -3.6370, -4.2447, -4.2964, -3.3246, -3.2044, -2.9079, -2.8080,\n",
       "         -2.7094, -4.0626, -3.7935, -3.8263, -3.1316, -2.8195, -3.1986, -3.6188,\n",
       "         -4.5389, -4.0627, -3.5916, -1.9032, -3.0939, -3.7700, -3.3071, -3.2362,\n",
       "         -3.2656, -3.6923, -3.5042],\n",
       "        [-3.6694, -3.8990, -3.2422, -3.4948, -2.5906, -4.0370, -2.7419, -3.5148,\n",
       "         -3.1858, -3.9286, -3.5002, -3.6154, -3.3074, -2.8598, -2.8353, -2.2577,\n",
       "         -3.5107, -3.4897, -4.2516, -2.8695, -3.7311, -4.4289, -3.4962, -3.7282,\n",
       "         -3.6700, -3.4965, -3.0940],\n",
       "        [-4.3913, -3.9713, -3.9221, -3.7816, -3.5042, -3.0065, -3.5313, -3.8209,\n",
       "         -2.6993, -3.9856, -3.6093, -4.1506, -3.0553, -3.5551, -2.8573, -2.0353,\n",
       "         -4.1972, -3.8282, -2.9320, -2.2770, -3.3669, -4.4250, -3.6840, -4.0447,\n",
       "         -3.0485, -3.6380, -3.0461],\n",
       "        [-3.5227, -3.6026, -2.9471, -3.3697, -2.3861, -4.1712, -3.1470, -3.3030,\n",
       "         -3.6055, -4.0242, -3.3527, -3.9409, -3.5792, -2.7699, -2.6545, -2.4680,\n",
       "         -3.8712, -3.5807, -4.7231, -3.1959, -3.3226, -4.1804, -3.2351, -3.8528,\n",
       "         -4.0458, -2.9809, -3.0931],\n",
       "        [-3.2552, -4.1750, -2.7114, -4.1695, -3.0404, -3.8049, -2.6176, -3.0350,\n",
       "         -2.7868, -3.7592, -4.1234, -3.6896, -3.5022, -3.7322, -3.5989, -2.6080,\n",
       "         -2.8489, -3.7107, -4.4385, -2.8454, -3.7330, -3.4383, -3.0002, -3.5459,\n",
       "         -3.1331, -3.6194, -3.3671],\n",
       "        [-3.2235, -3.9882, -3.8069, -2.8665, -3.1407, -3.3367, -2.8249, -2.7803,\n",
       "         -3.4868, -4.3118, -3.6290, -3.7792, -3.6138, -2.3541, -3.9087, -3.3222,\n",
       "         -3.7699, -3.6498, -4.3971, -2.3507, -3.1317, -3.6675, -3.7082, -3.4220,\n",
       "         -3.7345, -3.2831, -3.0679],\n",
       "        [-3.4177, -3.2386, -4.0392, -3.3766, -4.0973, -3.1604, -3.3042, -2.3555,\n",
       "         -3.3128, -4.5242, -3.7709, -3.9462, -3.5050, -3.6477, -4.2300, -4.2784,\n",
       "         -4.3276, -3.9574, -3.8496, -1.5683, -3.7124, -2.9932, -3.1667, -3.0396,\n",
       "         -3.2798, -4.0700, -3.4992],\n",
       "        [-4.0153, -4.2123, -3.4513, -4.2560, -2.7623, -3.2705, -3.0408, -3.5509,\n",
       "         -2.5995, -2.9656, -4.1311, -3.3113, -3.3179, -2.8823, -3.4032, -3.1868,\n",
       "         -2.9979, -3.4480, -3.1240, -3.2314, -2.9244, -4.1171, -3.1226, -3.6972,\n",
       "         -3.3363, -3.3047, -3.8091],\n",
       "        [-4.4750, -4.5652, -3.3860, -3.0828, -2.9184, -3.2755, -4.1639, -4.7325,\n",
       "         -2.9542, -3.4924, -3.7170, -4.6175, -2.9029, -3.3970, -2.8393, -2.1364,\n",
       "         -3.3791, -3.4702, -2.4782, -2.7167, -3.3270, -4.8387, -4.1805, -3.6787,\n",
       "         -3.2439, -3.3808, -3.5695],\n",
       "        [-3.8600, -3.8475, -3.6934, -2.0330, -3.8976, -2.8815, -2.7401, -2.9408,\n",
       "         -3.2332, -3.6966, -4.0118, -3.6180, -3.5512, -3.1615, -4.9229, -4.6403,\n",
       "         -3.5881, -3.4892, -4.0728, -2.2461, -2.9951, -3.6031, -4.1890, -3.3345,\n",
       "         -3.2007, -3.1728, -3.9539],\n",
       "        [-4.3790, -4.2178, -3.0895, -4.3225, -3.0968, -3.8700, -2.5365, -3.2171,\n",
       "         -2.7290, -3.4167, -4.2081, -3.6365, -2.9904, -3.2418, -3.7216, -3.1403,\n",
       "         -2.8366, -3.6915, -3.0334, -3.0648, -3.3887, -3.4104, -2.4797, -4.3088,\n",
       "         -3.5421, -3.4008, -3.5810],\n",
       "        [-4.0686, -3.5112, -3.2022, -4.0886, -3.2058, -2.8995, -3.7044, -4.2113,\n",
       "         -3.2309, -3.4294, -3.5662, -3.8189, -2.8297, -2.9968, -2.5623, -2.8156,\n",
       "         -3.7167, -3.6942, -2.6084, -3.4651, -3.0197, -4.8276, -3.9511, -3.0364,\n",
       "         -3.2169, -3.3864, -3.2091],\n",
       "        [-3.6959, -3.6578, -3.5737, -2.8415, -3.4496, -3.2562, -3.3828, -2.9342,\n",
       "         -2.6177, -3.8803, -3.8469, -3.7455, -2.8924, -2.9560, -3.2465, -3.7178,\n",
       "         -4.1622, -3.6914, -3.6280, -2.1011, -3.2623, -3.6337, -3.6045, -3.2343,\n",
       "         -3.5887, -3.7148, -3.7452],\n",
       "        [-3.5912, -3.0558, -3.4871, -4.5839, -3.3587, -3.0512, -3.0261, -3.9610,\n",
       "         -3.8286, -2.8508, -3.2619, -2.6281, -2.8242, -3.5860, -3.4227, -3.3994,\n",
       "         -3.0870, -3.8694, -3.2099, -3.7500, -2.9809, -4.3560, -3.7218, -2.5741,\n",
       "         -3.0725, -3.8270, -3.5300],\n",
       "        [-3.8770, -3.7485, -2.7374, -3.4259, -2.5027, -3.6450, -3.9064, -3.8345,\n",
       "         -3.4397, -4.0064, -3.5377, -4.1118, -3.3330, -2.4102, -2.5793, -2.5124,\n",
       "         -4.3216, -4.0339, -3.9948, -2.7777, -3.5145, -4.9006, -4.4329, -2.9738,\n",
       "         -3.6141, -3.3736, -2.8529],\n",
       "        [-3.1167, -2.7858, -3.0303, -3.7807, -2.6693, -3.8736, -3.4123, -3.0890,\n",
       "         -4.2145, -3.9055, -2.7299, -3.5513, -3.6250, -3.4338, -3.1785, -2.9449,\n",
       "         -3.8254, -3.2398, -4.3253, -2.7803, -3.3882, -3.6952, -3.8095, -3.8396,\n",
       "         -3.7493, -3.6178, -2.4836],\n",
       "        [-4.3790, -4.2178, -3.0895, -4.3225, -3.0968, -3.8700, -2.5365, -3.2171,\n",
       "         -2.7290, -3.4167, -4.2081, -3.6365, -2.9904, -3.2418, -3.7216, -3.1403,\n",
       "         -2.8366, -3.6915, -3.0334, -3.0648, -3.3887, -3.4104, -2.4797, -4.3088,\n",
       "         -3.5421, -3.4008, -3.5810],\n",
       "        [-4.3509, -3.7898, -3.5164, -4.1143, -2.9357, -3.3753, -2.9405, -3.1312,\n",
       "         -3.0150, -4.2271, -3.8311, -4.0051, -2.9042, -2.6753, -3.6807, -3.8939,\n",
       "         -3.9965, -4.5972, -3.5668, -1.7619, -3.4101, -3.9094, -3.0958, -3.0196,\n",
       "         -3.4397, -3.5434, -3.6768],\n",
       "        [-3.4571, -4.3154, -3.9714, -3.5909, -3.0137, -4.0808, -3.2751, -3.6715,\n",
       "         -2.7271, -3.6974, -3.0609, -3.6213, -2.8808, -3.1259, -2.8095, -2.1104,\n",
       "         -4.2170, -3.7076, -3.6972, -3.0626, -3.5053, -4.3437, -3.6101, -3.5673,\n",
       "         -3.3982, -3.1744, -2.9988],\n",
       "        [-3.4909, -2.7530, -3.1571, -3.1655, -3.2124, -3.2197, -3.3389, -3.1754,\n",
       "         -3.5918, -4.2586, -3.2393, -3.9134, -3.7412, -3.1092, -2.9203, -2.9450,\n",
       "         -4.2020, -3.7076, -3.8989, -1.9750, -4.2947, -4.1814, -4.0846, -3.3387,\n",
       "         -3.1998, -3.6874, -3.1639],\n",
       "        [-4.3790, -4.2178, -3.0895, -4.3225, -3.0968, -3.8700, -2.5365, -3.2171,\n",
       "         -2.7290, -3.4167, -4.2081, -3.6365, -2.9904, -3.2418, -3.7216, -3.1403,\n",
       "         -2.8366, -3.6915, -3.0334, -3.0648, -3.3887, -3.4104, -2.4797, -4.3088,\n",
       "         -3.5421, -3.4008, -3.5810],\n",
       "        [-3.4522, -4.2159, -3.6213, -4.7087, -2.4303, -3.5448, -2.5674, -3.5298,\n",
       "         -3.7249, -3.2571, -3.5079, -3.0173, -3.2919, -3.2034, -3.2401, -2.7449,\n",
       "         -2.6335, -4.0914, -3.3626, -3.6243, -4.0412, -4.2119, -2.6675, -3.8336,\n",
       "         -2.9771, -3.5430, -3.9645],\n",
       "        [-3.5655, -2.9988, -3.4954, -3.7508, -2.9042, -3.2692, -3.1672, -3.7827,\n",
       "         -4.1541, -3.3433, -2.4227, -3.7157, -3.4501, -3.7071, -2.9692, -3.1673,\n",
       "         -3.6717, -3.7144, -4.6689, -3.2705, -4.0620, -4.4566, -4.7739, -1.9852,\n",
       "         -3.0825, -4.0083, -2.8014],\n",
       "        [-3.1016, -3.7072, -3.9598, -4.4530, -2.8528, -3.2648, -2.8052, -3.7172,\n",
       "         -3.6099, -2.9218, -3.4095, -2.7868, -2.9856, -2.7865, -3.5841, -3.6141,\n",
       "         -3.3882, -3.4790, -3.3211, -3.8654, -3.4849, -4.7068, -2.9777, -2.8657,\n",
       "         -2.8829, -3.3601, -4.0895],\n",
       "        [-3.7025, -4.0398, -3.8314, -4.8313, -2.6112, -3.6931, -2.2141, -3.1852,\n",
       "         -3.5959, -3.3330, -4.0621, -3.0170, -2.7746, -3.2324, -3.5141, -3.5351,\n",
       "         -2.8652, -4.0220, -3.5346, -2.7954, -3.8904, -3.6848, -2.5107, -3.9437,\n",
       "         -3.2630, -3.6788, -4.2496],\n",
       "        [-3.9176, -3.7871, -3.0307, -4.2362, -2.7008, -3.4860, -3.4515, -3.4814,\n",
       "         -3.2634, -3.4162, -3.7746, -3.5593, -3.1659, -3.2300, -2.9457, -2.4652,\n",
       "         -3.0746, -3.5495, -2.8702, -3.4647, -2.7170, -4.3696, -3.3414, -4.2023,\n",
       "         -3.9182, -3.4188, -3.0353],\n",
       "        [-3.0503, -3.5115, -4.2207, -4.2814, -2.7231, -2.9319, -3.2303, -3.2772,\n",
       "         -4.3921, -3.8768, -3.0800, -3.5830, -3.5816, -3.4131, -3.3703, -3.1087,\n",
       "         -2.6954, -4.0806, -2.8181, -3.0300, -3.6543, -4.0290, -3.1755, -2.9152,\n",
       "         -2.4913, -4.4457, -3.6977],\n",
       "        [-4.1058, -4.1894, -3.3618, -3.9399, -2.5888, -3.9737, -2.6077, -3.5353,\n",
       "         -2.8272, -3.5707, -3.5157, -3.3348, -3.3145, -3.0316, -3.3931, -2.3379,\n",
       "         -3.4122, -3.6116, -3.4787, -3.2374, -3.2808, -4.1839, -3.1960, -4.0082,\n",
       "         -3.5524, -3.1299, -3.3328],\n",
       "        [-3.5752, -3.1773, -3.6858, -3.2212, -2.6836, -3.3171, -3.2458, -3.3345,\n",
       "         -3.8089, -4.1212, -3.1507, -3.7831, -3.9401, -3.2415, -4.1647, -3.0840,\n",
       "         -3.1788, -3.7606, -3.0600, -2.6129, -2.9679, -4.0565, -4.0865, -2.6114,\n",
       "         -2.7247, -3.9140, -3.4195],\n",
       "        [-3.4907, -3.7234, -2.8547, -3.9880, -2.3322, -3.1741, -3.7861, -3.4508,\n",
       "         -3.7345, -3.4357, -3.7744, -3.7177, -3.2357, -2.6293, -3.0816, -3.0171,\n",
       "         -3.1273, -3.6909, -2.9922, -3.3890, -3.2650, -4.7281, -2.8796, -3.8530,\n",
       "         -3.7140, -3.1936, -3.7375]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1837, -2.7647, -3.8949, -3.4204, -3.5088, -2.3889, -3.4442, -3.3123,\n",
       "         -4.0456, -3.9576, -3.0628, -3.4465, -3.0465, -3.2209, -2.9837, -3.9360,\n",
       "         -4.2538, -4.4107, -3.7310, -2.5961, -3.1221, -4.0849, -4.0183, -2.7534,\n",
       "         -2.7924, -3.7079, -3.6333],\n",
       "        [-3.3363, -3.0017, -2.3151, -3.2497, -2.7663, -3.9893, -3.7965, -3.6317,\n",
       "         -3.8491, -3.5038, -3.2904, -3.4172, -3.1890, -2.9364, -2.6960, -2.8940,\n",
       "         -3.6866, -3.9671, -4.3571, -2.9187, -3.7511, -4.6416, -4.3379, -2.6600,\n",
       "         -3.7620, -3.6106, -3.4701],\n",
       "        [-4.3264, -3.6370, -4.2447, -4.2964, -3.3246, -3.2044, -2.9079, -2.8080,\n",
       "         -2.7094, -4.0626, -3.7935, -3.8263, -3.1316, -2.8195, -3.1986, -3.6188,\n",
       "         -4.5389, -4.0627, -3.5916, -1.9032, -3.0939, -3.7700, -3.3071, -3.2362,\n",
       "         -3.2656, -3.6923, -3.5042],\n",
       "        [-3.6694, -3.8990, -3.2422, -3.4948, -2.5906, -4.0370, -2.7419, -3.5148,\n",
       "         -3.1858, -3.9286, -3.5002, -3.6154, -3.3074, -2.8598, -2.8353, -2.2577,\n",
       "         -3.5107, -3.4897, -4.2516, -2.8695, -3.7311, -4.4289, -3.4962, -3.7282,\n",
       "         -3.6700, -3.4965, -3.0940],\n",
       "        [-4.3913, -3.9713, -3.9221, -3.7816, -3.5042, -3.0065, -3.5313, -3.8209,\n",
       "         -2.6993, -3.9856, -3.6093, -4.1506, -3.0553, -3.5551, -2.8573, -2.0353,\n",
       "         -4.1972, -3.8282, -2.9320, -2.2770, -3.3669, -4.4250, -3.6840, -4.0447,\n",
       "         -3.0485, -3.6380, -3.0461],\n",
       "        [-3.5227, -3.6026, -2.9471, -3.3697, -2.3861, -4.1712, -3.1470, -3.3030,\n",
       "         -3.6055, -4.0242, -3.3527, -3.9409, -3.5792, -2.7699, -2.6545, -2.4680,\n",
       "         -3.8712, -3.5807, -4.7231, -3.1959, -3.3226, -4.1804, -3.2351, -3.8528,\n",
       "         -4.0458, -2.9809, -3.0931],\n",
       "        [-3.2552, -4.1750, -2.7114, -4.1695, -3.0404, -3.8049, -2.6176, -3.0350,\n",
       "         -2.7868, -3.7592, -4.1234, -3.6896, -3.5022, -3.7322, -3.5989, -2.6080,\n",
       "         -2.8489, -3.7107, -4.4385, -2.8454, -3.7330, -3.4383, -3.0002, -3.5459,\n",
       "         -3.1331, -3.6194, -3.3671],\n",
       "        [-3.2235, -3.9882, -3.8069, -2.8665, -3.1407, -3.3367, -2.8249, -2.7803,\n",
       "         -3.4868, -4.3118, -3.6290, -3.7792, -3.6138, -2.3541, -3.9087, -3.3222,\n",
       "         -3.7699, -3.6498, -4.3971, -2.3507, -3.1317, -3.6675, -3.7082, -3.4220,\n",
       "         -3.7345, -3.2831, -3.0679],\n",
       "        [-3.4177, -3.2386, -4.0392, -3.3766, -4.0973, -3.1604, -3.3042, -2.3555,\n",
       "         -3.3128, -4.5242, -3.7709, -3.9462, -3.5050, -3.6477, -4.2300, -4.2784,\n",
       "         -4.3276, -3.9574, -3.8496, -1.5683, -3.7124, -2.9932, -3.1667, -3.0396,\n",
       "         -3.2798, -4.0700, -3.4992],\n",
       "        [-4.0153, -4.2123, -3.4513, -4.2560, -2.7623, -3.2705, -3.0408, -3.5509,\n",
       "         -2.5995, -2.9656, -4.1311, -3.3113, -3.3179, -2.8823, -3.4032, -3.1868,\n",
       "         -2.9979, -3.4480, -3.1240, -3.2314, -2.9244, -4.1171, -3.1226, -3.6972,\n",
       "         -3.3363, -3.3047, -3.8091],\n",
       "        [-4.4750, -4.5652, -3.3860, -3.0828, -2.9184, -3.2755, -4.1639, -4.7325,\n",
       "         -2.9542, -3.4924, -3.7170, -4.6175, -2.9029, -3.3970, -2.8393, -2.1364,\n",
       "         -3.3791, -3.4702, -2.4782, -2.7167, -3.3270, -4.8387, -4.1805, -3.6787,\n",
       "         -3.2439, -3.3808, -3.5695],\n",
       "        [-3.8600, -3.8475, -3.6934, -2.0330, -3.8976, -2.8815, -2.7401, -2.9408,\n",
       "         -3.2332, -3.6966, -4.0118, -3.6180, -3.5512, -3.1615, -4.9229, -4.6403,\n",
       "         -3.5881, -3.4892, -4.0728, -2.2461, -2.9951, -3.6031, -4.1890, -3.3345,\n",
       "         -3.2007, -3.1728, -3.9539],\n",
       "        [-4.3790, -4.2178, -3.0895, -4.3225, -3.0968, -3.8700, -2.5365, -3.2171,\n",
       "         -2.7290, -3.4167, -4.2081, -3.6365, -2.9904, -3.2418, -3.7216, -3.1403,\n",
       "         -2.8366, -3.6915, -3.0334, -3.0648, -3.3887, -3.4104, -2.4797, -4.3088,\n",
       "         -3.5421, -3.4008, -3.5810],\n",
       "        [-4.0686, -3.5112, -3.2022, -4.0886, -3.2058, -2.8995, -3.7044, -4.2113,\n",
       "         -3.2309, -3.4294, -3.5662, -3.8189, -2.8297, -2.9968, -2.5623, -2.8156,\n",
       "         -3.7167, -3.6942, -2.6084, -3.4651, -3.0197, -4.8276, -3.9511, -3.0364,\n",
       "         -3.2169, -3.3864, -3.2091],\n",
       "        [-3.6959, -3.6578, -3.5737, -2.8415, -3.4496, -3.2562, -3.3828, -2.9342,\n",
       "         -2.6177, -3.8803, -3.8469, -3.7455, -2.8924, -2.9560, -3.2465, -3.7178,\n",
       "         -4.1622, -3.6914, -3.6280, -2.1011, -3.2623, -3.6337, -3.6045, -3.2343,\n",
       "         -3.5887, -3.7148, -3.7452],\n",
       "        [-3.5912, -3.0558, -3.4871, -4.5839, -3.3587, -3.0512, -3.0261, -3.9610,\n",
       "         -3.8286, -2.8508, -3.2619, -2.6281, -2.8242, -3.5860, -3.4227, -3.3994,\n",
       "         -3.0870, -3.8694, -3.2099, -3.7500, -2.9809, -4.3560, -3.7218, -2.5741,\n",
       "         -3.0725, -3.8270, -3.5300],\n",
       "        [-3.8770, -3.7485, -2.7374, -3.4259, -2.5027, -3.6450, -3.9064, -3.8345,\n",
       "         -3.4397, -4.0064, -3.5377, -4.1118, -3.3330, -2.4102, -2.5793, -2.5124,\n",
       "         -4.3216, -4.0339, -3.9948, -2.7777, -3.5145, -4.9006, -4.4329, -2.9738,\n",
       "         -3.6141, -3.3736, -2.8529],\n",
       "        [-3.1167, -2.7858, -3.0303, -3.7807, -2.6693, -3.8736, -3.4123, -3.0890,\n",
       "         -4.2145, -3.9055, -2.7299, -3.5513, -3.6250, -3.4338, -3.1785, -2.9449,\n",
       "         -3.8254, -3.2398, -4.3253, -2.7803, -3.3882, -3.6952, -3.8095, -3.8396,\n",
       "         -3.7493, -3.6178, -2.4836],\n",
       "        [-4.3790, -4.2178, -3.0895, -4.3225, -3.0968, -3.8700, -2.5365, -3.2171,\n",
       "         -2.7290, -3.4167, -4.2081, -3.6365, -2.9904, -3.2418, -3.7216, -3.1403,\n",
       "         -2.8366, -3.6915, -3.0334, -3.0648, -3.3887, -3.4104, -2.4797, -4.3088,\n",
       "         -3.5421, -3.4008, -3.5810],\n",
       "        [-4.3509, -3.7898, -3.5164, -4.1143, -2.9357, -3.3753, -2.9405, -3.1312,\n",
       "         -3.0150, -4.2271, -3.8311, -4.0051, -2.9042, -2.6753, -3.6807, -3.8939,\n",
       "         -3.9965, -4.5972, -3.5668, -1.7619, -3.4101, -3.9094, -3.0958, -3.0196,\n",
       "         -3.4397, -3.5434, -3.6768],\n",
       "        [-3.4571, -4.3154, -3.9714, -3.5909, -3.0137, -4.0808, -3.2751, -3.6715,\n",
       "         -2.7271, -3.6974, -3.0609, -3.6213, -2.8808, -3.1259, -2.8095, -2.1104,\n",
       "         -4.2170, -3.7076, -3.6972, -3.0626, -3.5053, -4.3437, -3.6101, -3.5673,\n",
       "         -3.3982, -3.1744, -2.9988],\n",
       "        [-3.4909, -2.7530, -3.1571, -3.1655, -3.2124, -3.2197, -3.3389, -3.1754,\n",
       "         -3.5918, -4.2586, -3.2393, -3.9134, -3.7412, -3.1092, -2.9203, -2.9450,\n",
       "         -4.2020, -3.7076, -3.8989, -1.9750, -4.2947, -4.1814, -4.0846, -3.3387,\n",
       "         -3.1998, -3.6874, -3.1639],\n",
       "        [-4.3790, -4.2178, -3.0895, -4.3225, -3.0968, -3.8700, -2.5365, -3.2171,\n",
       "         -2.7290, -3.4167, -4.2081, -3.6365, -2.9904, -3.2418, -3.7216, -3.1403,\n",
       "         -2.8366, -3.6915, -3.0334, -3.0648, -3.3887, -3.4104, -2.4797, -4.3088,\n",
       "         -3.5421, -3.4008, -3.5810],\n",
       "        [-3.4522, -4.2159, -3.6213, -4.7087, -2.4303, -3.5448, -2.5674, -3.5298,\n",
       "         -3.7249, -3.2571, -3.5079, -3.0173, -3.2919, -3.2034, -3.2401, -2.7449,\n",
       "         -2.6335, -4.0914, -3.3626, -3.6243, -4.0412, -4.2119, -2.6675, -3.8336,\n",
       "         -2.9771, -3.5430, -3.9645],\n",
       "        [-3.5655, -2.9988, -3.4954, -3.7508, -2.9042, -3.2692, -3.1672, -3.7827,\n",
       "         -4.1541, -3.3433, -2.4227, -3.7157, -3.4501, -3.7071, -2.9692, -3.1673,\n",
       "         -3.6717, -3.7144, -4.6689, -3.2705, -4.0620, -4.4566, -4.7739, -1.9852,\n",
       "         -3.0825, -4.0083, -2.8014],\n",
       "        [-3.1016, -3.7072, -3.9598, -4.4530, -2.8528, -3.2648, -2.8052, -3.7172,\n",
       "         -3.6099, -2.9218, -3.4095, -2.7868, -2.9856, -2.7865, -3.5841, -3.6141,\n",
       "         -3.3882, -3.4790, -3.3211, -3.8654, -3.4849, -4.7068, -2.9777, -2.8657,\n",
       "         -2.8829, -3.3601, -4.0895],\n",
       "        [-3.7025, -4.0398, -3.8314, -4.8313, -2.6112, -3.6931, -2.2141, -3.1852,\n",
       "         -3.5959, -3.3330, -4.0621, -3.0170, -2.7746, -3.2324, -3.5141, -3.5351,\n",
       "         -2.8652, -4.0220, -3.5346, -2.7954, -3.8904, -3.6848, -2.5107, -3.9437,\n",
       "         -3.2630, -3.6788, -4.2496],\n",
       "        [-3.9176, -3.7871, -3.0307, -4.2362, -2.7008, -3.4860, -3.4515, -3.4814,\n",
       "         -3.2634, -3.4162, -3.7746, -3.5593, -3.1659, -3.2300, -2.9457, -2.4652,\n",
       "         -3.0746, -3.5495, -2.8702, -3.4647, -2.7170, -4.3696, -3.3414, -4.2023,\n",
       "         -3.9182, -3.4188, -3.0353],\n",
       "        [-3.0503, -3.5115, -4.2207, -4.2814, -2.7231, -2.9319, -3.2303, -3.2772,\n",
       "         -4.3921, -3.8768, -3.0800, -3.5830, -3.5816, -3.4131, -3.3703, -3.1087,\n",
       "         -2.6954, -4.0806, -2.8181, -3.0300, -3.6543, -4.0290, -3.1755, -2.9152,\n",
       "         -2.4913, -4.4457, -3.6977],\n",
       "        [-4.1058, -4.1894, -3.3618, -3.9399, -2.5888, -3.9737, -2.6077, -3.5353,\n",
       "         -2.8272, -3.5707, -3.5157, -3.3348, -3.3145, -3.0316, -3.3931, -2.3379,\n",
       "         -3.4122, -3.6116, -3.4787, -3.2374, -3.2808, -4.1839, -3.1960, -4.0082,\n",
       "         -3.5524, -3.1299, -3.3328],\n",
       "        [-3.5752, -3.1773, -3.6858, -3.2212, -2.6836, -3.3171, -3.2458, -3.3345,\n",
       "         -3.8089, -4.1212, -3.1507, -3.7831, -3.9401, -3.2415, -4.1647, -3.0840,\n",
       "         -3.1788, -3.7606, -3.0600, -2.6129, -2.9679, -4.0565, -4.0865, -2.6114,\n",
       "         -2.7247, -3.9140, -3.4195],\n",
       "        [-3.4907, -3.7234, -2.8547, -3.9880, -2.3322, -3.1741, -3.7861, -3.4508,\n",
       "         -3.7345, -3.4357, -3.7744, -3.7177, -3.2357, -2.6293, -3.0816, -3.0171,\n",
       "         -3.1273, -3.6909, -2.9922, -3.3890, -3.2650, -4.7281, -2.8796, -3.8530,\n",
       "         -3.7140, -3.1936, -3.7375]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
